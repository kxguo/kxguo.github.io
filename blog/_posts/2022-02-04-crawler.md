---
layout: post
title: 爬取orv并制作epub
tags: [crawler,Data Analysis]
image:
  path: /assets/img/blog/orv.jpeg
description: >
  用爬虫爬取外网小说omniscient reader's view并制作成epub阅读
sitemap: false
---

快速定位：[爬虫快速指南](#爬虫快速指南)
, [re匹配正则表达式](#re匹配正则表达式) 
, [爬取过程](#爬取过程)
, [pandoc格式转换](#pandoc格式转换)

- Table of Contents
{:toc .large-only}

## 爬虫快速指南

~~~python
from bs4 import BeautifulSoup #网页解析，获取数据
import re  #正则表达式，文字匹配
import urllib.request, urllib.error #制定url，获取网页数据
~~~

### 1. 无条件获取一个get请求
~~~python
response=urllib.request.urlopen("http://www.baidu.com")
print(response.read().decode('utf-8')) #对获取到的网页源码进行utf-8源码
~~~

### 2. 获取一个post请求
用于模拟浏览器的真实请求，表单后可以填写用户信息
~~~python
import urllib.parse
data=bytes(urllib.parse.urlencode({"name":"eric"}),encoding="utf-8")
response=urllib.request.urlopen("http://httpbin.org/post",data=data)
print(response.read().decode("utf-8"))
~~~

### 3. 获取get请求的超时处理
~~~python
try:
    response=urllib.request.urlopen("http://httpbin.org/get",timeout=0.5)
    print(response.read().decode("utf-8"))
except urllib.error.URLError as e:
    print("time out!")

response=urllib.request.urlopen("http://baidu.com")
print(response.status) 
# 正常状态200;对豆瓣则显示418错误：代表爬虫已被发现,需要复制浏览器Headers的user-agent
print(response.getheaders())
# mac操作：command+option+i -> network -> stop -> headers -> user-agent
# 得到user-agent 
~~~

### 4. 模拟浏览器的user_agent的post请求
~~~python
url="http://httpbin.org/post"
headers={
"user-agent": "[user-agent]"
}
req=urllib.request.Request(url=url,data=data,headers=headers,method="POST") #构建请求对象
response=urllib.request.urlopen(req) #利用对象进行传输获取响应
print(response.read().decode("utf-8"))
~~~

### 5. 只模拟浏览器，不需要传输表单(即data)的情形
用get，如爬取豆瓣
~~~python
url="https://www.douban.com"
headers={
"user-agent": "[user-agent]"
}
req=urllib.request.Request(url=url,headers=headers)
response=urllib.request.urlopen(req)
print(response.read().decode("utf-8"))
~~~
### 6. 利用bs进行文本解析
BeautifulSoup4将html文档转换成一个复杂的树形结构，每个节点都是Python对象，所有对象可以归纳为4种
- Tag
- Navigable String
- Beautiful Soup
- Comment
~~~python
from bs4 import BeautifulSoup
req=urllib.request.Request(url="http://baidu.com",headers=headers)
response=urllib.request.urlopen(req)
print(response.status)
html=response.read().decode("utf-8")
bs=BeautifulSoup(html,"html.parser") #用html.parser解析器来解析html文档，形成树形结构
print(bs.title)
print(type(bs.title))
#以上1.Tag 标签及其内容：返回第一个内容
print(bs.title.string)
print(type(bs.title.string))
#以上2.标签里的文字内容（字符串）
print(bs.a.attrs) #字典，可得到a下面class/href等所有的属性
print(bs.a.attrs['class'])
print(type(bs))
print(bs.name)
#以上3.自身类型，表示整个文档
#内容中有<!--新闻-->时，string的类型是comment，返回值不含注释符号
#以上4.Comment，是一个特殊的navigableString
~~~

### 7. 文档遍历
~~~python
print(bs.head.contents)
~~~

### 8. 文档定位
#### find_all()
- 字符串过滤：查找所有与字符串完全匹配的标签（如<a>），而不是第一个
~~~python
t_list=bs.find_all("a")
~~~
- 正则表达式搜索：使用search()方法匹配标签内容
~~~python
import re
t_list=bs.find_all(re.compile("a"))
~~~
- 方法：传入一个函数，根据函数要求搜索
~~~python
def name_is_exists(tag):
    return tag.has_attr("name")
t_list=bs.find_all(name_is_exists) #有name属性的所有标签
~~~

#### kwargs参数
~~~python
t_list=bs.find_all(href="http://new.baidu.com")
t_list=bs.find_all(id="head") #id=head的所有内容
t_list=bs.find_all(class_=True) #有class属性的所有内容
t_list=bs.find_all(text=["hao123","地图","贴吧"])
#t_list=bs.find_all(text=re.compile("[A-z]\d")) #正则表达式来查找包含特定文本的内容（标签里的字符串）
t_list=bs.find_all("a",limit=3) #限制前三个结果
~~~

#### CSS选择器
~~~python
t_list=bs.select('title') #通过标签查找
t_list=bs.select(".mnav") #按类（class）名查找
t_list=bs.select("#u1") #通过Id查找
t_list=bs.select("a[class='bri']") #通过属性查找
t_list=bs.select("link>title") #通过限定父标签查找
#t_list=bs.select(".mnav ~ .bri") #通过兄弟节点查找
for item in t_list:
    print(item.get_text()) #取得文本
~~~

## re匹配正则表达式
正则表达式：字符串模式（判断字符串是否符合一定的标准）

### 创建模式对象查找
~~~python
pat=re.compile("AA") #此处“AA”代表正则表达式，用来验证其他字符串
m=pat.search("CBA") #"CBA"为被校验的内容
#print(m)
#>>none
m=pat.search("ABCAA")
#print(m)
#>><re.Match object; span=(3, 5), match='AA'>
m=pat.search("ABCAADDCCAAA")
#print(m)
#>><re.Match object; span=(3, 5), match='AA'>
#search只返回第一次匹配的位置
~~~

### 没有模式对象查找匹配
~~~python
m=re.search("asd","Aasd") #前面字符串是模式模板，后面的是被校验的对象
#print(m)
#>><re.Match object; span=(1, 4), match='asd'>
print(re.findall("a","ASDaDFGAa")) #前为模板，后位被校验字符串
#>>['a', 'a']
#findall返回全部符合标准的字符串
print(re.findall("[A-Z]","ASDaDFGAa"))
#>>['A', 'S', 'D', 'D', 'F', 'G', 'A']
print(re.findall("[A-Z]+","ASDaDFGAa"))
#>>['ASD', 'DFGA']
~~~

### 模式替换
~~~python
print(re.sub("a","A","abcdcasd")) #1-被替换对象 2-替换对象 3-原字符串
#>>AbcdcAsd
#建议在正则表达式中被比较字符前加上r，避免特殊字符被转义,如
a=r"\aabd-\'"
print(a)
#>>\aabd-\
~~~

## 爬取过程

### 功能模块
~~~python
from bs4 import BeautifulSoup #网页解析，获取数据
import re  #正则表达式，文字匹配
import urllib.request, urllib.error #制定url，获取网页数据
import unicodedata
import time

#得到指定一个url的网页内容
def askURL(url):
    head={"user-agent":\
          "Mozilla/5.0 (Macintosh; \
          Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) \
          Chrome/103.0.0.0 Safari/537.36"
}
    request=urllib.request.Request(url,headers=head)
    html=""
    try:
        response=urllib.request.urlopen(request)
        html=response.read().decode("utf-8")
    except urllib.error.URLError as e:
        if hasattr(e,"code"):
            #错误代码
            print(e.code)
        if hasattr(e,"reason"):
            #未捕获成功的原因
            print(e.reason)
    return html

#获取章节中的文本
def getText(baseurl,pretitle):
    textlist=[]
    html=askURL(baseurl)
    bs=BeautifulSoup(html,"html.parser")
    seccontent=bs.select("div[class='text-left']>p")
    for i in seccontent:
        if not i.script:
            content=unicodedata.normalize("NFKD", i.get_text().strip())
            #处理md转义字符
            tmp=content.replace("#", "\#")
            content=tmp.replace("*","\*")
            tmp=content.replace("<","\<")
            content=tmp.replace(">","\>")
            #在章节标题前加一级标题md符号
            if len(textlist)==0 and len(re.findall("Episode|Prologue|Epilogue|Chapter",content,flags=re.I))>0\
            and not 'TL' in content and not 'Early chapters' in content:
                textlist.append("# "+content+'\n\n')
                #f.write("# "+content+'\n\n')
            #没有自带标题的情况
            elif len(textlist)==0:
                textlist.append("# "+pretitle+'\n\n')
                textlist.append(content+'\n\n')
            else:
                textlist.append(content+'\n\n')
                #f.write(content+'\n\n')
            #textlist.append(content)
    return textlist

#爬取网页列表中的数据
def getData(baseurl):
    pagelist=[]
    titlelist=[]
    html=askURL(baseurl) #保存获取到的网页源码
    #逐一解析数据
    soup=BeautifulSoup(html,"html.parser")
    for item in soup.find_all('li',class_='wp-manga-chapter'):
        sechtml=item.a.attrs['href']
        sectitle=item.a.get_text().strip()
        pagelist.append(sechtml)
        titlelist.append(sectitle)
    pagelist.reverse() #原网页章节排序为倒序
    titlelist.reverse()
    contents=[]
    #读取各章节列表
    for index in range(len(pagelist)):
        chapter=pagelist[index]
        title=titlelist[index]
        chap=getText(chapter,title)
        contents+=chap
        print(index)
        if index%30==0: #每抓取30章休息30秒
#             break
            time.sleep(30)
    savepath="./chapterlist/content.txt"
    f=open(savepath,"w")
    for line in contents:
        line=line.replace('“','\"')
        line=line.replace('”','\"')
        f.write(line)
    f.close()
    return contents
~~~
### 根据url爬取章节数据
~~~python
baseurl="https://webnovelonline.net/read/omniscient-readers-viewpoint/"
datalist=getData(baseurl)
~~~
爬取的数据已按照MarkDown格式存入`content.txt`，为了微调数据，保存datalist变量
~~~python
import dill
filename='ovr_variable.pkl'
~~~

## pandoc格式转换
下载封面图片(可以为任意格式）orv.webp, 建立meta文件orvmeta
~~~shell
# file: `ovrmeta`
---
title: Ominicent Reader’s Viewpoint
---
~~~
运行pandoc
~~~shell
pandoc -f gfm -s --toc ./chapterlist/*.txt --metadata-file=orvmeta --epub-cover-image=orv.webp -o orv.epub
~~~

![Full-width image](/assets/img/blog/blurred_orv.png)
